{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snake_env import Snake\n",
    "from snake_env_12in import SnakeEnv\n",
    "from snake_env_12in_turn import SnakeEnvTurn\n",
    "import datetime\n",
    "import random\n",
    "import numpy as np\n",
    "from keras import Sequential\n",
    "from collections import deque\n",
    "from keras.layers import Dense\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.optimizers import Adam\n",
    "from plot_script import plot_result\n",
    "from edit import BasicEnv\n",
    "import time\n",
    "\n",
    "\n",
    "class DQN:\n",
    "\n",
    "    \"\"\" Deep Q Network \"\"\"\n",
    "\n",
    "    def __init__(self, env, params):\n",
    "\n",
    "        self.action_space = env.action_space\n",
    "        self.state_space = env.state_space\n",
    "        self.epsilon = params['epsilon'] \n",
    "        self.gamma = params['gamma'] \n",
    "        self.batch_size = params['batch_size'] \n",
    "        self.epsilon_min = params['epsilon_min'] \n",
    "        self.epsilon_decay = params['epsilon_decay'] \n",
    "        self.learning_rate = params['learning_rate']\n",
    "        self.layer_sizes = params['layer_sizes']\n",
    "        self.memory = deque(maxlen=2500)\n",
    "        self.model = self.build_model()\n",
    "\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        for i in range(len(self.layer_sizes)):\n",
    "            if i == 0:\n",
    "                model.add(Dense(self.layer_sizes[i], input_shape=(self.state_space,), activation='relu'))\n",
    "            else:\n",
    "                model.add(Dense(self.layer_sizes[i], activation='relu'))\n",
    "        model.add(Dense(self.action_space, activation='softmax'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate),metrics='mae')\n",
    "        print(model.summary())\n",
    "        return model\n",
    "\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "\n",
    "    def act(self, state):\n",
    "\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_space)\n",
    "        act_values = self.model.predict(state,verbose=0)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "\n",
    "    def replay(self):\n",
    "\n",
    "        if len(self.memory) < 2:\n",
    "            return [0],[0]\n",
    "        \n",
    "        if len(self.memory) < self.batch_size:\n",
    "            l = len(self.memory)\n",
    "        else:\n",
    "            l = self.batch_size\n",
    "            \n",
    "        minibatch = random.sample(self.memory, l)\n",
    "        states = np.array([i[0] for i in minibatch])\n",
    "        actions = np.array([i[1] for i in minibatch])\n",
    "        rewards = np.array([i[2] for i in minibatch])\n",
    "        next_states = np.array([i[3] for i in minibatch])\n",
    "        dones = np.array([i[4] for i in minibatch])\n",
    "\n",
    "        states = np.squeeze(states)\n",
    "        next_states = np.squeeze(next_states)\n",
    "\n",
    "        targets = rewards + self.gamma*(np.amax(self.model.predict_on_batch(next_states), axis=1))*(1-dones)\n",
    "        targets_full = self.model.predict_on_batch(states)\n",
    "\n",
    "        ind = np.array([i for i in range(l)])\n",
    "        targets_full[[ind], [actions]] = targets\n",
    "\n",
    "        history = self.model.fit(states, targets_full, epochs=1, verbose=0)\n",
    "        loss = history.history['loss']\n",
    "        mae = history.history['mae']\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        return loss, mae\n",
    "\n",
    "\n",
    "def train_dqn(episode, env,agent):\n",
    "\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    train_log_dir = 'logs/dqn/' + current_time + '/train'\n",
    "    train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "    sum_of_rewards = []\n",
    "    \n",
    "    for e in range(episode):\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, (1, env.state_space))\n",
    "        score = 0\n",
    "        max_steps = 10000\n",
    "        food_count = 0\n",
    "        for i in range(max_steps):\n",
    "            action = agent.act(state)\n",
    "            # print(action)\n",
    "            prev_state = state\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            if reward == 10:\n",
    "                food_count += 1\n",
    "            score += reward\n",
    "            next_state = np.reshape(next_state, (1, env.state_space))\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            if params['batch_size'] > 1:\n",
    "                loss,mae = agent.replay()\n",
    "            if done:\n",
    "                print(f'final state before dying: {str(prev_state)}')\n",
    "                print(f'episode: {e+1}/{episode}, score: {score}')\n",
    "                break\n",
    "        episode_steps = i\n",
    "        episode_reward = score\n",
    "        with train_summary_writer.as_default():\n",
    "            tf.summary.scalar(\"Total episode reward\",episode_reward,step=e)\n",
    "            tf.summary.scalar(\"Total episode steps\",episode_steps,step=e)\n",
    "            tf.summary.scalar(\"Food eaten in episode\",food_count,step=e)\n",
    "            tf.summary.scalar(\"mae\",mae[0],step=e)\n",
    "            tf.summary.scalar(\"loss\",loss[0],step=e)\n",
    "        \n",
    "        sum_of_rewards.append(score)\n",
    "    agent.model.save_weights(\"500batch_monster3.h5f\")\n",
    "    return sum_of_rewards\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 128)               1664      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 35,075\n",
      "Trainable params: 35,075\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\Anaconda3\\envs\\snake-game\\lib\\site-packages\\keras\\optimizers\\legacy\\adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "params = dict()\n",
    "params['name'] = None\n",
    "params['epsilon'] = 0.01\n",
    "params['gamma'] = .95\n",
    "params['batch_size'] = 500\n",
    "params['epsilon_min'] = .01\n",
    "params['epsilon_decay'] = .995\n",
    "params['learning_rate'] = 0.00025\n",
    "params['layer_sizes'] = [128, 128, 128]\n",
    "\n",
    "results = dict()\n",
    "ep = 50\n",
    "\n",
    "# for batchsz in [1, 10, 100, 1000]:\n",
    "#     print(batchsz)\n",
    "#     params['batch_size'] = batchsz\n",
    "#     nm = ''\n",
    "#     params['name'] = f'Batchsize {batchsz}'\n",
    "env_infos = {'States: only walls':{'state_space':'no body knowledge'}, 'States: direction 0 or 1':{'state_space':''}, 'States: coordinates':{'state_space':'coordinates'}, 'States: no direction':{'state_space':'no direction'}}\n",
    "\n",
    "# for key in env_infos.keys():\n",
    "#     params['name'] = key\n",
    "#     env_info = env_infos[key]\n",
    "#     print(env_info)\n",
    "#     env = Snake(env_info=env_info)\n",
    "env_info = {\"state_space\" : \"no direction\"}\n",
    "# env = SnakeEnv((20, 20),(10,10),1)\n",
    "env = SnakeEnvTurn((20, 20),(10,10),1)\n",
    "# env = Snake()\n",
    "agent = DQN(env, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pretrain \n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"state_and_action_data.csv\")\n",
    "\n",
    "data_x = df.copy().drop([\"action_left\",\"action_forward\",\"action_right\"],axis=1)\n",
    "data_y = df[[\"action_left\",\"action_forward\",\"action_right\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>food_up</th>\n",
       "      <th>food_down</th>\n",
       "      <th>food_right</th>\n",
       "      <th>food_left</th>\n",
       "      <th>obs_up</th>\n",
       "      <th>obs_down</th>\n",
       "      <th>obs_left</th>\n",
       "      <th>obs_right</th>\n",
       "      <th>dir_up</th>\n",
       "      <th>dir_down</th>\n",
       "      <th>dir_right</th>\n",
       "      <th>dir_left</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   food_up  food_down  food_right  food_left  obs_up  obs_down  obs_left   \n",
       "0        0          1           1          0       0         0         0  \\\n",
       "1        0          0           1          0       0         0         0   \n",
       "2        1          0           1          0       0         0         0   \n",
       "3        1          0           1          0       0         0         0   \n",
       "4        1          0           1          0       0         0         0   \n",
       "\n",
       "   obs_right  dir_up  dir_down  dir_right  dir_left  \n",
       "0          0       0         1          0         0  \n",
       "1          0       0         1          0         0  \n",
       "2          0       0         1          0         0  \n",
       "3          0       0         1          0         0  \n",
       "4          0       0         0          0         1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = data_x.to_numpy()\n",
    "ys = data_y.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.1421 - mae: 0.3179\n",
      "Epoch 2/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0903 - mae: 0.1801\n",
      "Epoch 3/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0864 - mae: 0.1840\n",
      "Epoch 4/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0816 - mae: 0.1799\n",
      "Epoch 5/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0776 - mae: 0.1677\n",
      "Epoch 6/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0752 - mae: 0.1596\n",
      "Epoch 7/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0739 - mae: 0.1529\n",
      "Epoch 8/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0732 - mae: 0.1496\n",
      "Epoch 9/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0729 - mae: 0.1493\n",
      "Epoch 10/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0728 - mae: 0.1475\n",
      "Epoch 11/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0725 - mae: 0.1474\n",
      "Epoch 12/100\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.0725 - mae: 0.1469\n",
      "Epoch 13/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0724 - mae: 0.1469\n",
      "Epoch 14/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0724 - mae: 0.1461\n",
      "Epoch 15/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0723 - mae: 0.1464\n",
      "Epoch 16/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0723 - mae: 0.1458\n",
      "Epoch 17/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0722 - mae: 0.1463\n",
      "Epoch 18/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0724 - mae: 0.1452\n",
      "Epoch 19/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0722 - mae: 0.1457\n",
      "Epoch 20/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0721 - mae: 0.1459\n",
      "Epoch 21/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0722 - mae: 0.1451\n",
      "Epoch 22/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0722 - mae: 0.1454\n",
      "Epoch 23/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0721 - mae: 0.1455\n",
      "Epoch 24/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0722 - mae: 0.1459\n",
      "Epoch 25/100\n",
      "60/60 [==============================] - 0s 4ms/step - loss: 0.0724 - mae: 0.1463\n",
      "Epoch 26/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0721 - mae: 0.1447\n",
      "Epoch 27/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0722 - mae: 0.1446\n",
      "Epoch 28/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0720 - mae: 0.1453\n",
      "Epoch 29/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0721 - mae: 0.1455\n",
      "Epoch 30/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0720 - mae: 0.1454\n",
      "Epoch 31/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0720 - mae: 0.1443\n",
      "Epoch 32/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0721 - mae: 0.1451\n",
      "Epoch 33/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0721 - mae: 0.1447\n",
      "Epoch 34/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0720 - mae: 0.1448\n",
      "Epoch 35/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0719 - mae: 0.1451\n",
      "Epoch 36/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0720 - mae: 0.1445\n",
      "Epoch 37/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0721 - mae: 0.1448\n",
      "Epoch 38/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0721 - mae: 0.1442\n",
      "Epoch 39/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0719 - mae: 0.1443\n",
      "Epoch 40/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0720 - mae: 0.1441\n",
      "Epoch 41/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0720 - mae: 0.1455\n",
      "Epoch 42/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0719 - mae: 0.1450\n",
      "Epoch 43/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0721 - mae: 0.1448\n",
      "Epoch 44/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0720 - mae: 0.1454\n",
      "Epoch 45/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0720 - mae: 0.1450\n",
      "Epoch 46/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0720 - mae: 0.1445\n",
      "Epoch 47/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0720 - mae: 0.1441\n",
      "Epoch 48/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0719 - mae: 0.1445\n",
      "Epoch 49/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0720 - mae: 0.1446\n",
      "Epoch 50/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0721 - mae: 0.1450\n",
      "Epoch 51/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0719 - mae: 0.1444\n",
      "Epoch 52/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0719 - mae: 0.1447\n",
      "Epoch 53/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0719 - mae: 0.1443\n",
      "Epoch 54/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0719 - mae: 0.1453\n",
      "Epoch 55/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0720 - mae: 0.1446\n",
      "Epoch 56/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0719 - mae: 0.1448\n",
      "Epoch 57/100\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.0719 - mae: 0.1449\n",
      "Epoch 58/100\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.0719 - mae: 0.1444\n",
      "Epoch 59/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0721 - mae: 0.1460\n",
      "Epoch 60/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0720 - mae: 0.1447\n",
      "Epoch 61/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0718 - mae: 0.1451\n",
      "Epoch 62/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0719 - mae: 0.1453\n",
      "Epoch 63/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0720 - mae: 0.1443\n",
      "Epoch 64/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0719 - mae: 0.1442\n",
      "Epoch 65/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0720 - mae: 0.1449\n",
      "Epoch 66/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0718 - mae: 0.1451\n",
      "Epoch 67/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0719 - mae: 0.1433\n",
      "Epoch 68/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0720 - mae: 0.1440\n",
      "Epoch 69/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0719 - mae: 0.1444\n",
      "Epoch 70/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0719 - mae: 0.1433\n",
      "Epoch 71/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0719 - mae: 0.1439\n",
      "Epoch 72/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0719 - mae: 0.1451\n",
      "Epoch 73/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0719 - mae: 0.1456\n",
      "Epoch 74/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0719 - mae: 0.1441\n",
      "Epoch 75/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0718 - mae: 0.1438\n",
      "Epoch 76/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0720 - mae: 0.1436\n",
      "Epoch 77/100\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.0718 - mae: 0.1433\n",
      "Epoch 78/100\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.0720 - mae: 0.1438\n",
      "Epoch 79/100\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.0720 - mae: 0.1450\n",
      "Epoch 80/100\n",
      "60/60 [==============================] - 0s 4ms/step - loss: 0.0718 - mae: 0.1437\n",
      "Epoch 81/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0718 - mae: 0.1437\n",
      "Epoch 82/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0718 - mae: 0.1444\n",
      "Epoch 83/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0719 - mae: 0.1442\n",
      "Epoch 84/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0718 - mae: 0.1443\n",
      "Epoch 85/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0719 - mae: 0.1445\n",
      "Epoch 86/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0719 - mae: 0.1449\n",
      "Epoch 87/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0718 - mae: 0.1433\n",
      "Epoch 88/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0718 - mae: 0.1443\n",
      "Epoch 89/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0718 - mae: 0.1431\n",
      "Epoch 90/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0718 - mae: 0.1446\n",
      "Epoch 91/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0719 - mae: 0.1433\n",
      "Epoch 92/100\n",
      "27/60 [============>.................] - ETA: 0s - loss: 0.0712 - mae: 0.1448"
     ]
    }
   ],
   "source": [
    "agent.model.fit(xs,ys,batch_size=500,epochs=100,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.callbacks import TensorBoard\n",
    "# from tensorflow import keras\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir logs/dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_memory = pd.read_csv(\"memory_data.csv\")\n",
    "sample = df_memory.sample(2500)\n",
    "df_memory.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_memory = pd.read_csv(\"memory_data_elite.csv\")\n",
    "sample = df_memory.sample(2500)\n",
    "df_memory.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "get_state = lambda i : np.array(json.loads(df_memory.iloc[i]['state'].replace(' ',',')))\n",
    "get_nstate = lambda i : np.array(json.loads(df_memory.iloc[i]['next_state'].replace(' ',',')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2500):\n",
    "    state = get_state(i)\n",
    "    state = np.reshape(state, (1, env.state_space))\n",
    "    nstate = get_nstate(i)\n",
    "    nstate = np.reshape(nstate, (1, env.state_space))\n",
    "    action = df_memory.iloc[i]['actions']\n",
    "    reward = df_memory.iloc[i]['reward']\n",
    "    done = df_memory.iloc[i]['done']\n",
    "    agent.memory.append(((state, action, reward, nstate, done)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_of_rewards = train_dqn(ep, env,agent=agent)\n",
    "results[params['name']] = sum_of_rewards\n",
    "\n",
    "plot_result(results, direct=True, k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snake-game",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
